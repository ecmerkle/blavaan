<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Model Comparison • blavaan</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Model Comparison">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">blavaan</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.5-5.1296</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-basics" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Basics</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-basics">
<li><a class="dropdown-item" href="../articles/start.html">Getting Started</a></li>
    <li><a class="dropdown-item" href="../articles/prior.html">Prior Specification</a></li>
    <li><a class="dropdown-item" href="../articles/estimate.html">Estimation</a></li>
    <li><a class="dropdown-item" href="../articles/convergence_efficiency.html">Convergence and Efficiency Evaluation</a></li>
    <li><a class="dropdown-item" href="../articles/summaries.html">Model Summaries</a></li>
    <li><a class="dropdown-item" href="../articles/plotting.html">Plots</a></li>
  </ul>
</li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-examples-details" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Examples/Details</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-examples-details">
<li><a class="dropdown-item" href="../articles/ordinal.html">Estimation with Ordinal Data</a></li>
    <li><a class="dropdown-item" href="../articles/multilevel.html">Two-level Estimation</a></li>
    <li><a class="dropdown-item" href="../articles/invariance.html">Measurement Invariance</a></li>
    <li><a class="dropdown-item" href="../articles/approx_fi.html">Approximate Fit Indices</a></li>
    <li><a class="dropdown-item" href="../articles/model_comparison.html">Model Comparison</a></li>
    <li><a class="dropdown-item" href="../articles/cross_loadings_strong_priors.html">Cross-loadings with Strong Priors</a></li>
    <li><a class="dropdown-item" href="../articles/mod_indices.html">Modification Indices</a></li>
    <li><a class="dropdown-item" href="../articles/prior_pred_checks.html">Prior Predictive Checks</a></li>
    <li><a class="dropdown-item" href="../articles/convergence_loop.html">Convergence Loop</a></li>
    <li><a class="dropdown-item" href="../articles/probability_direction.html">Probability of Direction</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">News</a></li>
<li class="nav-item"><a class="nav-link" href="../articles/resources.html">Resources</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Functions</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/ecmerkle/blavaan"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://groups.google.com/d/forum/blavaan"><span class="fa fa-users"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Model Comparison</h1>
                        <h4 data-toc-skip class="author">Mauricio
Garnier-Villarreal</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/ecmerkle/blavaan/blob/master/vignettes/model_comparison.Rmd" class="external-link"><code>vignettes/model_comparison.Rmd</code></a></small>
      <div class="d-none name"><code>model_comparison.Rmd</code></div>
    </div>

    
    
<div class="section level3">
<h3 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h3>
<p>The traditional method for model comparison in frequentist SEM (fSEM)
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>χ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
(Likelihood Ratio Test) and its variations. But for BSEM, we would take
the Bayesian model comparison methods, and apply them to SEM.</p>
<p>Specifically, we will focus on two information criteria, (1) Widely
Applicable Information Criterion (WAIC), and (2) Leave-One-Out
cross-validation (LOO).</p>
<p>These methods intend to evaluate the out-of-sample predictive
accuracy of the models, and compare that performance. This is the
ability to predict a datapoint that hasn’t been used in the
<strong>training</strong> model <span class="citation">(McElreath
2020)</span></p>
<p>For this example we will use the Industrialization and Political
Democracy example <span class="citation">(Bollen 1989)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="st">'</span></span>
<span><span class="st">  # latent variable definitions</span></span>
<span><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span><span class="st">     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4</span></span>
<span><span class="st">     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8</span></span>
<span><span class="st"></span></span>
<span><span class="st">  # regressions</span></span>
<span><span class="st">    dem60 ~ ind60</span></span>
<span><span class="st">    dem65 ~ ind60 + dem60</span></span>
<span><span class="st"></span></span>
<span><span class="st">  # residual correlations</span></span>
<span><span class="st">    y1 ~~ y5</span></span>
<span><span class="st">    y2 ~~ y4 + y6</span></span>
<span><span class="st">    y3 ~~ y7</span></span>
<span><span class="st">    y4 ~~ y8</span></span>
<span><span class="st">    y6 ~~ y8</span></span>
<span><span class="st">'</span></span>
<span></span>
<span><span class="va">fit1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bsem.html">bsem</a></span><span class="op">(</span><span class="va">model</span>, data<span class="op">=</span><span class="va">PoliticalDemocracy</span>,</span>
<span>            std.lv<span class="op">=</span><span class="cn">T</span>, meanstructure<span class="op">=</span><span class="cn">T</span>, n.chains<span class="op">=</span><span class="fl">3</span>,</span>
<span>            burnin<span class="op">=</span><span class="fl">500</span>, sample<span class="op">=</span><span class="fl">1000</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="widely-applicable-information-criterion">Widely Applicable Information Criterion<a class="anchor" aria-label="anchor" href="#widely-applicable-information-criterion"></a>
</h3>
<p>WAIC <span class="citation">(Watanabe 2010)</span> can be seen as a
fully Bayesian generalization of the Akaike Information Criteria (AIC),
where we have a measure of uncertainty/information of the model
prediction for each row in the data across all posterior draws. This is
the Log-Pointwise-Predictive-Density (lppd). The WAIC is defined as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mi>A</mi><mi>I</mi><mi>C</mi><mo>=</mo><mo>−</mo><mn>2</mn><mi>l</mi><mi>p</mi><mi>p</mi><mi>d</mi><mo>+</mo><mn>2</mn><mi>e</mi><mi>f</mi><msub><mi>p</mi><mrow><mi>W</mi><mi>A</mi><mi>I</mi><mi>C</mi></mrow></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}
WAIC= -2lppd + 2efp_{WAIC},
\end{equation}</annotation></semantics></math> The first term involves
the log-likelihoods of observed data (marginal over latent variables)
and the second term is the effective number of parameters. The first
term,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>p</mi><mi>p</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">lppd</annotation></semantics></math>,
is estimated as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mrow><mi>l</mi><mi>p</mi><mi>p</mi><mi>d</mi></mrow><mo accent="true">̂</mo></mover><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>l</mi><mi>o</mi><mi>g</mi><mo minsize="3.0" maxsize="3.0" stretchy="false" form="prefix">(</mo><mfrac><mn>1</mn><mi>S</mi></mfrac><munderover><mo>∑</mo><mrow><mi>S</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msup><mi>θ</mi><mi>S</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo minsize="3.0" maxsize="3.0" stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{equation}
\widehat{lppd} = \sum^{n}_{i = 1} log \Bigg(\frac{1}{S}\sum^{S}_{S=1}f(y_{i}|\theta^{S}) \Bigg)
\end{equation}</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
is the number of posterior draws and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msup><mi>θ</mi><mi>S</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(y_{i}|\theta^{S})</annotation></semantics></math>
is the density of observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
with respect to the parameter sampled at iteration
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>.</p>
<p>The effective number of parameter
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mi>f</mi><msub><mi>p</mi><mrow><mi>W</mi><mi>A</mi><mi>I</mi><mi>C</mi></mrow></msub></mrow><annotation encoding="application/x-tex">efp_{WAIC}</annotation></semantics></math>)
is calculated as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mi>f</mi><msub><mi>p</mi><mrow><mi>W</mi><mi>A</mi><mi>I</mi><mi>C</mi></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>v</mi><mi>a</mi><msub><mi>r</mi><mi>s</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation}\label{eq:efpWAIC}
efp_{WAIC} = \sum^n_{i=1}var_{s}(logf(y_{i}|\theta))
\end{equation}</annotation></semantics></math></p>
<p>A separate variance is estimated for each observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
across the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
posterior draws.</p>
</div>
<div class="section level3">
<h3 id="leave-one-out-cross-validation">Leave-One-Out cross-validation<a class="anchor" aria-label="anchor" href="#leave-one-out-cross-validation"></a>
</h3>
<p>The LOO measures the predictive density of each observation holding
out one observation at the time and use the rest of the observations to
update the prior. This estimation is calculated via <span class="citation">(Vehtari, Gelman, and Gabry 2017)</span>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>O</mi><mi>O</mi><mo>=</mo><mo>−</mo><mn>2</mn><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi>l</mi><mi>o</mi><mi>g</mi><mo minsize="3.0" maxsize="3.0" stretchy="false" form="prefix">(</mo><mfrac><mrow><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msubsup><mi>w</mi><mi>i</mi><mi>s</mi></msubsup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false" form="prefix">|</mo><msup><mi>θ</mi><mi>s</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><msubsup><mi>w</mi><mi>i</mi><mi>s</mi></msubsup></mrow></mfrac><mo minsize="3.0" maxsize="3.0" stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\begin{equation}
    LOO = -2\sum_{i=1}^{n} log \Bigg(\frac{\sum^{S}_{s =1} w^{s}_{i}f(y_{i}|\theta^{s})}{\sum^{s}_{s=1} w^{s}_{i}}\Bigg)
\end{equation}</annotation></semantics></math></p>
<p>Where the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>w</mi><mi>i</mi><mi>s</mi></msubsup><annotation encoding="application/x-tex">w^s_{i}</annotation></semantics></math>
are Pareto-smoothed sampling weights based on the relative magnitude of
individual
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
density function across the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
posterior samples.</p>
<p>The LOO effective number of parameters involves the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>p</mi><mi>p</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">lppd</annotation></semantics></math>
term from WAIC:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mi>f</mi><msub><mi>p</mi><mrow><mi>L</mi><mi>O</mi><mi>O</mi></mrow></msub><mo>=</mo><mi>l</mi><mi>p</mi><mi>p</mi><mi>d</mi><mo>+</mo><mi>L</mi><mi>O</mi><mi>O</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\begin{equation}
    efp_{LOO} = lppd + LOO/2
\end{equation}</annotation></semantics></math></p>
</div>
<div class="section level3">
<h3 id="model-comparison">Model comparison<a class="anchor" aria-label="anchor" href="#model-comparison"></a>
</h3>
<p>As both WAIC and LOO approximate the models’ performance across
posterior draws, we are able to calculate a standard error for them and
for model comparisons involving them.</p>
<p>The model differences estimate the differences across the Expected
Log-Pointwise-Predictive-Density (elpd), and the standard error of the
respective difference.</p>
<p>There are no clear cutoff rules on how to interpret and present these
comparisons, and the researchers need to use their expert knowledge as
part of the decision process. The best recommendation is to present the
differences in elpd
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>e</mi><mi>l</mi><mi>p</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">\Delta elpd</annotation></semantics></math>,
the standard error, and the ratio between them. If the ratio is at least
2 can be consider evidence of differences between the models, and a
ratio of 4 would be considered stronger evidence.</p>
<p>For the first example, we will compare the standard political
democracy model, with a model where all factor regressions are fixed to
0.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="st">'</span></span>
<span><span class="st">  # latent variable definitions</span></span>
<span><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span><span class="st">     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4</span></span>
<span><span class="st">     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8</span></span>
<span><span class="st"></span></span>
<span><span class="st">  # regressions</span></span>
<span><span class="st">    dem60 ~ 0*ind60</span></span>
<span><span class="st">    dem65 ~ 0*ind60 + 0*dem60</span></span>
<span><span class="st"></span></span>
<span><span class="st">  # residual correlations</span></span>
<span><span class="st">    y1 ~~ y5</span></span>
<span><span class="st">    y2 ~~ y4 + y6</span></span>
<span><span class="st">    y3 ~~ y7</span></span>
<span><span class="st">    y4 ~~ y8</span></span>
<span><span class="st">    y6 ~~ y8</span></span>
<span><span class="st">'</span></span>
<span></span>
<span><span class="va">fit2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bsem.html">bsem</a></span><span class="op">(</span><span class="va">model</span>, data<span class="op">=</span><span class="va">PoliticalDemocracy</span>,</span>
<span>            std.lv<span class="op">=</span><span class="cn">T</span>, meanstructure<span class="op">=</span><span class="cn">T</span>, n.chains<span class="op">=</span><span class="fl">3</span>,</span>
<span>            burnin<span class="op">=</span><span class="fl">500</span>, sample<span class="op">=</span><span class="fl">1000</span><span class="op">)</span></span></code></pre></div>
<p>Once we have the 2 models, we can compare them with the
<code>blavCompare</code></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bc12</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/blavCompare.html">blavCompare</a></span><span class="op">(</span><span class="va">fit1</span>, <span class="va">fit2</span><span class="op">)</span></span></code></pre></div>
<p>By looking into this comparison object, you can see the WAIC, LOO,
estimates, and the respective differences between them. As these are
information criteria, the <strong>best</strong> model is the one with
the lowest value</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bc12</span></span></code></pre></div>
<pre><code><span><span class="co">## $bf</span></span>
<span><span class="co">##   bf mll1 mll2 </span></span>
<span><span class="co">##   NA   NA   NA </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $loo</span></span>
<span><span class="co">## $loo[[1]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##          Estimate   SE</span></span>
<span><span class="co">## elpd_loo  -1605.8 19.5</span></span>
<span><span class="co">## p_loo        37.4  2.9</span></span>
<span><span class="co">## looic      3211.6 39.1</span></span>
<span><span class="co">## ------</span></span>
<span><span class="co">## MCSE of elpd_loo is 0.2.</span></span>
<span><span class="co">## MCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.3]).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## All Pareto k estimates are good (k &lt; 0.7).</span></span>
<span><span class="co">## See help('pareto-k-diagnostic') for details.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $loo[[2]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##          Estimate   SE</span></span>
<span><span class="co">## elpd_loo  -1647.1 18.9</span></span>
<span><span class="co">## p_loo        34.8  2.8</span></span>
<span><span class="co">## looic      3294.3 37.8</span></span>
<span><span class="co">## ------</span></span>
<span><span class="co">## MCSE of elpd_loo is 0.2.</span></span>
<span><span class="co">## MCSE and ESS estimates assume MCMC draws (r_eff in [0.3, 1.2]).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## All Pareto k estimates are good (k &lt; 0.7).</span></span>
<span><span class="co">## See help('pareto-k-diagnostic') for details.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $diff_loo</span></span>
<span><span class="co">##        elpd_diff se_diff</span></span>
<span><span class="co">## model1   0.0       0.0  </span></span>
<span><span class="co">## model2 -41.3       7.9  </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $waic</span></span>
<span><span class="co">## $waic[[1]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##           Estimate   SE</span></span>
<span><span class="co">## elpd_waic  -1605.5 19.5</span></span>
<span><span class="co">## p_waic        37.2  2.9</span></span>
<span><span class="co">## waic        3211.1 39.0</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 39 (52.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $waic[[2]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##           Estimate   SE</span></span>
<span><span class="co">## elpd_waic  -1646.9 18.8</span></span>
<span><span class="co">## p_waic        34.5  2.7</span></span>
<span><span class="co">## waic        3293.7 37.7</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 33 (44.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $diff_waic</span></span>
<span><span class="co">##        elpd_diff se_diff</span></span>
<span><span class="co">## model1   0.0       0.0  </span></span>
<span><span class="co">## model2 -41.3       7.9</span></span></code></pre>
<p>In this case we can see that model 1 has lower LOOIC, and the ratio
shows that the LOO differences is 5 SE of magnitude. This indicates that
the model with the estimated regressions is better</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">bc12</span><span class="op">$</span><span class="va">diff_loo</span><span class="op">[</span>,<span class="st">"elpd_diff"</span><span class="op">]</span> <span class="op">/</span> <span class="va">bc12</span><span class="op">$</span><span class="va">diff_loo</span><span class="op">[</span>,<span class="st">"se_diff"</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##   model1   model2 </span></span>
<span><span class="co">##      NaN 5.247424</span></span></code></pre>
<p>Now, lets look at an example with a smaller difference between
models, where only the smallest regression (<code>dem65~ind60</code>) is
fixed to 0.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="st">'</span></span>
<span><span class="st">  # latent variable definitions</span></span>
<span><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span><span class="st">     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4</span></span>
<span><span class="st">     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8</span></span>
<span><span class="st"></span></span>
<span><span class="st">  # regressions</span></span>
<span><span class="st">    dem60 ~ ind60</span></span>
<span><span class="st">    dem65 ~ 0*ind60 + dem60</span></span>
<span><span class="st"></span></span>
<span><span class="st">  # residual correlations</span></span>
<span><span class="st">    y1 ~~ y5</span></span>
<span><span class="st">    y2 ~~ y4 + y6</span></span>
<span><span class="st">    y3 ~~ y7</span></span>
<span><span class="st">    y4 ~~ y8</span></span>
<span><span class="st">    y6 ~~ y8</span></span>
<span><span class="st">'</span></span>
<span></span>
<span><span class="va">fit3</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bsem.html">bsem</a></span><span class="op">(</span><span class="va">model</span>, data<span class="op">=</span><span class="va">PoliticalDemocracy</span>,</span>
<span>            std.lv<span class="op">=</span><span class="cn">T</span>, meanstructure<span class="op">=</span><span class="cn">T</span>, n.chains<span class="op">=</span><span class="fl">3</span>,</span>
<span>            burnin<span class="op">=</span><span class="fl">500</span>, sample<span class="op">=</span><span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">bc13</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/blavCompare.html">blavCompare</a></span><span class="op">(</span><span class="va">fit1</span>, <span class="va">fit3</span><span class="op">)</span></span></code></pre></div>
<p>When we see the LOOIC, we see that the difference between the two
models is minimal, and the ratio is 0.21. This indicates that the models
are functionally equivalent. In a case like this, it is up to the
researchers to decide which model is a <strong>better</strong>
representation, and theoretically stronger.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bc13</span></span></code></pre></div>
<pre><code><span><span class="co">## $bf</span></span>
<span><span class="co">##   bf mll1 mll2 </span></span>
<span><span class="co">##   NA   NA   NA </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $loo</span></span>
<span><span class="co">## $loo[[1]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##          Estimate   SE</span></span>
<span><span class="co">## elpd_loo  -1605.8 19.5</span></span>
<span><span class="co">## p_loo        37.4  2.9</span></span>
<span><span class="co">## looic      3211.6 39.1</span></span>
<span><span class="co">## ------</span></span>
<span><span class="co">## MCSE of elpd_loo is 0.2.</span></span>
<span><span class="co">## MCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.3]).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## All Pareto k estimates are good (k &lt; 0.7).</span></span>
<span><span class="co">## See help('pareto-k-diagnostic') for details.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $loo[[2]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##          Estimate   SE</span></span>
<span><span class="co">## elpd_loo  -1606.5 19.5</span></span>
<span><span class="co">## p_loo        37.3  2.9</span></span>
<span><span class="co">## looic      3213.0 39.0</span></span>
<span><span class="co">## ------</span></span>
<span><span class="co">## MCSE of elpd_loo is NA.</span></span>
<span><span class="co">## MCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.3]).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Pareto k diagnostic values:</span></span>
<span><span class="co">##                          Count Pct.    Min. ESS</span></span>
<span><span class="co">## (-Inf, 0.7]   (good)     74    98.7%   150     </span></span>
<span><span class="co">##    (0.7, 1]   (bad)       1     1.3%   &lt;NA&gt;    </span></span>
<span><span class="co">##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    </span></span>
<span><span class="co">## See help('pareto-k-diagnostic') for details.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $diff_loo</span></span>
<span><span class="co">##        elpd_diff se_diff</span></span>
<span><span class="co">## model1  0.0       0.0   </span></span>
<span><span class="co">## model2 -0.7       1.0   </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $waic</span></span>
<span><span class="co">## $waic[[1]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##           Estimate   SE</span></span>
<span><span class="co">## elpd_waic  -1605.5 19.5</span></span>
<span><span class="co">## p_waic        37.2  2.9</span></span>
<span><span class="co">## waic        3211.1 39.0</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 39 (52.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $waic[[2]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##           Estimate   SE</span></span>
<span><span class="co">## elpd_waic  -1606.1 19.4</span></span>
<span><span class="co">## p_waic        36.9  2.8</span></span>
<span><span class="co">## waic        3212.2 38.9</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 37 (49.3%) p_waic estimates greater than 0.4. We recommend trying loo instead. </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $diff_waic</span></span>
<span><span class="co">##        elpd_diff se_diff</span></span>
<span><span class="co">## model1  0.0       0.0   </span></span>
<span><span class="co">## model2 -0.6       0.9</span></span></code></pre>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">bc13</span><span class="op">$</span><span class="va">diff_loo</span><span class="op">[</span>,<span class="st">"elpd_diff"</span><span class="op">]</span> <span class="op">/</span> <span class="va">bc13</span><span class="op">$</span><span class="va">diff_loo</span><span class="op">[</span>,<span class="st">"se_diff"</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##    model1    model2 </span></span>
<span><span class="co">##       NaN 0.7048045</span></span></code></pre>
<p>Lets do one last model, where only the largest regression
(<code>dem65~dem60</code>) is fixed to 0.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">model</span> <span class="op">&lt;-</span> <span class="st">'</span></span>
<span><span class="st">  # latent variable definitions</span></span>
<span><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span><span class="st">     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4</span></span>
<span><span class="st">     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8</span></span>
<span><span class="st"></span></span>
<span><span class="st">  # regressions</span></span>
<span><span class="st">    dem60 ~ ind60</span></span>
<span><span class="st">    dem65 ~ ind60 + 0*dem60</span></span>
<span><span class="st"></span></span>
<span><span class="st">  # residual correlations</span></span>
<span><span class="st">    y1 ~~ y5</span></span>
<span><span class="st">    y2 ~~ y4 + y6</span></span>
<span><span class="st">    y3 ~~ y7</span></span>
<span><span class="st">    y4 ~~ y8</span></span>
<span><span class="st">    y6 ~~ y8</span></span>
<span><span class="st">'</span></span>
<span></span>
<span><span class="va">fit4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bsem.html">bsem</a></span><span class="op">(</span><span class="va">model</span>, data<span class="op">=</span><span class="va">PoliticalDemocracy</span>,</span>
<span>            std.lv<span class="op">=</span><span class="cn">T</span>, meanstructure<span class="op">=</span><span class="cn">T</span>, n.chains<span class="op">=</span><span class="fl">3</span>,</span>
<span>            burnin<span class="op">=</span><span class="fl">500</span>, sample<span class="op">=</span><span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">bc14</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/blavCompare.html">blavCompare</a></span><span class="op">(</span><span class="va">fit1</span>, <span class="va">fit4</span><span class="op">)</span></span></code></pre></div>
<p>In this case, by looking at the LOOIC, we see that model one is
better (lower value), and the ratio of the difference shows that the
model is 5 SE in magnitude. Indicating that there is evidence of model
predictive differences</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">bc14</span></span></code></pre></div>
<pre><code><span><span class="co">## $bf</span></span>
<span><span class="co">##   bf mll1 mll2 </span></span>
<span><span class="co">##   NA   NA   NA </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $loo</span></span>
<span><span class="co">## $loo[[1]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##          Estimate   SE</span></span>
<span><span class="co">## elpd_loo  -1605.8 19.5</span></span>
<span><span class="co">## p_loo        37.4  2.9</span></span>
<span><span class="co">## looic      3211.6 39.1</span></span>
<span><span class="co">## ------</span></span>
<span><span class="co">## MCSE of elpd_loo is 0.2.</span></span>
<span><span class="co">## MCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.3]).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## All Pareto k estimates are good (k &lt; 0.7).</span></span>
<span><span class="co">## See help('pareto-k-diagnostic') for details.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $loo[[2]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##          Estimate   SE</span></span>
<span><span class="co">## elpd_loo  -1629.2 19.9</span></span>
<span><span class="co">## p_loo        37.7  3.0</span></span>
<span><span class="co">## looic      3258.5 39.9</span></span>
<span><span class="co">## ------</span></span>
<span><span class="co">## MCSE of elpd_loo is NA.</span></span>
<span><span class="co">## MCSE and ESS estimates assume MCMC draws (r_eff in [0.6, 1.3]).</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Pareto k diagnostic values:</span></span>
<span><span class="co">##                          Count Pct.    Min. ESS</span></span>
<span><span class="co">## (-Inf, 0.7]   (good)     74    98.7%   476     </span></span>
<span><span class="co">##    (0.7, 1]   (bad)       1     1.3%   &lt;NA&gt;    </span></span>
<span><span class="co">##    (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    </span></span>
<span><span class="co">## See help('pareto-k-diagnostic') for details.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $diff_loo</span></span>
<span><span class="co">##        elpd_diff se_diff</span></span>
<span><span class="co">## model1   0.0       0.0  </span></span>
<span><span class="co">## model2 -23.4       4.1  </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $waic</span></span>
<span><span class="co">## $waic[[1]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##           Estimate   SE</span></span>
<span><span class="co">## elpd_waic  -1605.5 19.5</span></span>
<span><span class="co">## p_waic        37.2  2.9</span></span>
<span><span class="co">## waic        3211.1 39.0</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 39 (52.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $waic[[2]]</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Computed from 3000 by 75 log-likelihood matrix.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##           Estimate   SE</span></span>
<span><span class="co">## elpd_waic  -1628.9 19.9</span></span>
<span><span class="co">## p_waic        37.4  2.9</span></span>
<span><span class="co">## waic        3257.9 39.8</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 38 (50.7%) p_waic estimates greater than 0.4. We recommend trying loo instead. </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## $diff_waic</span></span>
<span><span class="co">##        elpd_diff se_diff</span></span>
<span><span class="co">## model1   0.0       0.0  </span></span>
<span><span class="co">## model2 -23.4       4.1</span></span></code></pre>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="va">bc14</span><span class="op">$</span><span class="va">diff_loo</span><span class="op">[</span>,<span class="st">"elpd_diff"</span><span class="op">]</span> <span class="op">/</span> <span class="va">bc14</span><span class="op">$</span><span class="va">diff_loo</span><span class="op">[</span>,<span class="st">"se_diff"</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##  model1  model2 </span></span>
<span><span class="co">##     NaN 5.71708</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="bayes-factor">Bayes factor<a class="anchor" aria-label="anchor" href="#bayes-factor"></a>
</h3>
<p>In the Bayesian literature you will make use of the Bayes factor (BF)
to compare models. There are a number of criticisms related to the use
of the BF in BSEM, including (1) the BF is unstable for large models
(like most SEMs), (2) it is highly sensitive to model priors, (3) it
requires strong priors to have stable estimation of it, (4) it can
require large number of posterior draws, (5) the estimation using the
marginal likelihood ignores a lot of information from the posterior
distributions. For more details on this discussion please see <span class="citation">Tendeiro and Kiers (2019)</span> and <span class="citation">Schad et al. (2022)</span>. These criticisms lead us to
recommend against use of the BF in everyday BSEM estimation. For
researchers who commit to their prior distributions and who commit to
exploring the noise in their computations, the BF can used to describe
the relative odds of one model over another, which is more intuitive
than some other model comparison metrics.</p>
</div>
<div class="section level3">
<h3 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h3>
<p>We recommend the use of LOO or WAIC as general model comparison
metrics for BSEM. They allow us to estimate the models’ out-of-sample
predictive accuracy, and the respective differences across posterior
draws. They also provide us uncertainty estimates in the comparison.</p>
<p>In most cases LOO and WAIC will lead to similar results, and LOO is
recommended as the most stable metric <span class="citation">(Vehtari,
Gelman, and Gabry 2017)</span>. In general, a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>e</mi><mi>l</mi><mi>p</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">\Delta elpd</annotation></semantics></math>
of at least 2 standard errors and preferably 4 standard errors can be
interpreted as evidence of differential predictive accuracy.</p>
</div>
<div class="section level3 unnumbered">
<h3 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h3>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-bollen_structural_1989" class="csl-entry">
Bollen, Kenneth A. 1989. <em>Structural <span>Equations</span> with
<span>Latent</span> <span>Variables</span></em>. Wiley Series in
Probability and Mathematical Statistics. John Wiley &amp; Sons, Inc.
</div>
<div id="ref-mcelreath_statistical_2020" class="csl-entry">
McElreath, Richard. 2020. <em>Statistical Rethinking: A
<span>Bayesian</span> Course with Examples in <span>R</span> and
<span>Stan</span></em>. 2nd ed. <span>CRC</span> Texts in Statistical
Science. Boca Raton: Taylor; Francis, CRC Press.
</div>
<div id="ref-schad_workflow_2022" class="csl-entry">
Schad, Daniel J., Bruno Nicenboim, Paul-Christian Bürkner, Michael
Betancourt, and Shravan Vasishth. 2022. <span>“Workflow Techniques for
the Robust Use of Bayes Factors.”</span> <em>Psychological Methods</em>,
March. <a href="https://doi.org/10.1037/met0000472" class="external-link">https://doi.org/10.1037/met0000472</a>.
</div>
<div id="ref-tendeiro_review_2019" class="csl-entry">
Tendeiro, Jorge N., and Henk A. L. Kiers. 2019. <span>“A Review of
Issues about Null Hypothesis <span>Bayesian</span> Testing.”</span>
<em>Psychological Methods</em> 24 (6): 774–95. <a href="https://doi.org/10.1037/met0000221" class="external-link">https://doi.org/10.1037/met0000221</a>.
</div>
<div id="ref-vehtari_practical_2017" class="csl-entry">
Vehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. <span>“Practical
<span>Bayesian</span> Model Evaluation Using Leave-One-Out
Cross-Validation and <span>WAIC</span>.”</span> <em>Statistics and
Computing</em> 27 (5): 1413–32. <a href="https://doi.org/10.1007/s11222-016-9696-4" class="external-link">https://doi.org/10.1007/s11222-016-9696-4</a>.
</div>
<div id="ref-watanabeAsymptoticEquivalenceBayesa" class="csl-entry">
Watanabe, Sumio. 2010. <span>“Asymptotic Equivalence of Bayes Cross
Validation and Widely Applicable Information Criterion in Singular
Learning Theory.”</span> <em>Journal of Machine Learning Research</em>
11: 3571–94.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Edgar Merkle, Yves Rosseel, Ben Goodrich.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
